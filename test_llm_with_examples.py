#!/usr/bin/env python3
"""
Test LLM Strategy Generation with Real Examples

Tests the complete workflow:
1. Load real strategies from database
2. Build enhanced prompts with examples
3. Generate new strategy using LLM
4. Validate generated code
5. Compare quality
"""

import sys
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO, format="%(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Add to path
sys.path.insert(0, str(Path(__file__).parent))

from exhaustionlab.app.llm.enhanced_prompts import (
    EnhancedPromptBuilder,
    create_enhanced_strategy_prompt,
)
from exhaustionlab.app.llm.llm_client import LocalLLMClient
from exhaustionlab.app.llm.prompts import PromptContext


def test_prompt_generation():
    """Test enhanced prompt generation."""
    logger.info("=" * 70)
    logger.info("TEST 1: Enhanced Prompt Generation")
    logger.info("=" * 70)

    builder = EnhancedPromptBuilder()

    # Get statistics
    stats = builder.get_statistics()
    logger.info(f"\nüìä Database Stats:")
    logger.info(f"  Total with code: {stats['total_with_code']}")
    logger.info(f"  Avg quality: {stats['avg_quality']}")
    logger.info(f"  Avg LOC: {stats['avg_loc']}")

    # Create momentum strategy prompt
    logger.info(f"\nüìù Creating momentum strategy prompt...")

    context = PromptContext(
        strategy_type="strategy",
        market_focus=["spot", "futures"],
        timeframe="15m",
        indicators_to_include=["RSI", "MACD", "EMA"],
        signal_logic="momentum",
        risk_profile="balanced",
    )

    prompt = builder.build_strategy_prompt(
        context, include_examples=True, num_examples=2
    )

    logger.info(f"  ‚úÖ Prompt generated: {len(prompt)} chars")
    logger.info(f"  Contains real examples: {'REAL STRATEGY EXAMPLES' in prompt}")
    logger.info(f"  Contains RSI: {'RSI' in prompt}")
    logger.info(f"  Contains MACD: {'MACD' in prompt}")

    # Show sample
    logger.info(f"\nüìÑ Prompt Sample (first 500 chars):")
    logger.info("-" * 70)
    logger.info(prompt[:500])
    logger.info("...")

    return prompt


def test_llm_connection():
    """Test LLM API connection."""
    logger.info("\n" + "=" * 70)
    logger.info("TEST 2: LLM Connection")
    logger.info("=" * 70)

    client = LocalLLMClient()

    logger.info(f"\nüîó Testing connection to: {client.base_url}")
    logger.info(f"   Model: {client.model_name}")

    connected = client.test_connection()

    if connected:
        logger.info(f"  ‚úÖ LLM API is accessible!")
        return True
    else:
        logger.warning(f"  ‚ö†Ô∏è LLM API not accessible")
        logger.warning(f"     Make sure LM Studio is running on {client.base_url}")
        return False


def test_strategy_generation(prompt: str, llm_connected: bool):
    """Test actual strategy generation."""
    logger.info("\n" + "=" * 70)
    logger.info("TEST 3: Strategy Generation")
    logger.info("=" * 70)

    if not llm_connected:
        logger.warning("‚ö†Ô∏è Skipping - LLM not connected")
        logger.info("\nTo test generation:")
        logger.info("1. Start LM Studio")
        logger.info("2. Load DeepSeek R1 model")
        logger.info("3. Start server on http://127.0.0.1:1234")
        logger.info("4. Run this script again")
        return None

    client = LocalLLMClient()

    logger.info(f"\nü§ñ Generating strategy with LLM...")
    logger.info(f"   Temperature: 0.7")
    logger.info(f"   Max tokens: 2000")

    try:
        # Build LLM request
        from exhaustionlab.app.llm.llm_client import LLMRequest

        request = LLMRequest(
            prompt=prompt,
            system_prompt="You are an expert Pine Script and PyneCore developer.",
            temperature=0.7,
            max_tokens=2000,
        )

        # Generate
        logger.info(f"   Sending request...")
        response = client.generate(request)

        if response.success:
            logger.info(f"  ‚úÖ Generation successful!")
            logger.info(f"     Content length: {len(response.content)} chars")
            logger.info(f"     Code blocks found: {len(response.code_blocks)}")
            logger.info(f"     Request time: {response.request_time:.2f}s")

            if response.usage:
                logger.info(f"     Tokens used: {response.usage}")

            # Show sample of generated code
            if response.code_blocks:
                logger.info(f"\nüìÑ Generated Code Sample (first 500 chars):")
                logger.info("-" * 70)
                logger.info(response.code_blocks[0][:500])
                logger.info("...")

            return response
        else:
            logger.error(f"  ‚ùå Generation failed: {response.error_message}")
            return None

    except Exception as e:
        logger.error(f"  ‚ùå Error during generation: {e}")
        return None


def test_validation(response):
    """Test code validation."""
    logger.info("\n" + "=" * 70)
    logger.info("TEST 4: Code Validation")
    logger.info("=" * 70)

    if not response or not response.code_blocks:
        logger.warning("‚ö†Ô∏è No code to validate")
        return

    from exhaustionlab.app.llm.validators import PyneCoreValidator

    validator = PyneCoreValidator()
    code = response.code_blocks[0]

    logger.info(f"\nüîç Validating generated code...")

    # Syntax validation
    logger.info(f"   Checking syntax...")
    syntax_valid = validator.validate_syntax(code)
    logger.info(f"   {'‚úÖ' if syntax_valid else '‚ùå'} Syntax validation")

    # Structure validation
    logger.info(f"   Checking structure...")
    structure = validator.validate_structure(code)
    logger.info(f"   Has decorator: {structure.get('has_decorator', False)}")
    logger.info(f"   Has inputs: {structure.get('has_inputs', False)}")
    logger.info(f"   Has main logic: {structure.get('has_main_logic', False)}")

    # API usage validation
    logger.info(f"   Checking API usage...")
    api_valid = validator.validate_api_usage(code)
    logger.info(f"   {'‚úÖ' if api_valid else '‚ùå'} API validation")

    # Overall score
    score = validator.calculate_quality_score(code)
    logger.info(f"\nüìä Overall Quality Score: {score}/100")

    if score >= 70:
        logger.info(f"   ‚úÖ EXCELLENT - Production ready!")
    elif score >= 50:
        logger.info(f"   ‚ö†Ô∏è GOOD - Needs minor improvements")
    else:
        logger.info(f"   ‚ùå POOR - Needs major improvements")


def compare_with_examples():
    """Compare generation quality with database examples."""
    logger.info("\n" + "=" * 70)
    logger.info("TEST 5: Comparison with Database")
    logger.info("=" * 70)

    from exhaustionlab.app.meta_evolution.strategy_database import StrategyDatabase

    db = StrategyDatabase()
    strategies = db.search(has_code=True, min_quality_score=60, limit=5)

    logger.info(f"\nüìä Top 5 Database Strategies:")
    for i, s in enumerate(strategies, 1):
        logger.info(f"\n{i}. {s.name}")
        logger.info(f"   Quality: {s.quality_score:.1f}")
        logger.info(f"   LOC: {s.lines_of_code}")
        logger.info(
            f"   Indicators: {', '.join(s.indicators_used) if s.indicators_used else 'N/A'}"
        )


def main():
    """Run all tests."""
    logger.info("\nüöÄ LLM INTEGRATION TEST WITH REAL EXAMPLES\n")

    try:
        # Test 1: Prompt generation
        prompt = test_prompt_generation()

        # Test 2: LLM connection
        llm_connected = test_llm_connection()

        # Test 3: Strategy generation (if LLM available)
        response = test_strategy_generation(prompt, llm_connected)

        # Test 4: Validation (if generated)
        if response:
            test_validation(response)

        # Test 5: Compare with examples
        compare_with_examples()

        # Final summary
        logger.info("\n" + "=" * 70)
        logger.info("‚úÖ INTEGRATION TEST COMPLETE")
        logger.info("=" * 70)

        logger.info(f"\nüìä Summary:")
        logger.info(f"  ‚úÖ Enhanced prompts: WORKING")
        logger.info(f"  ‚úÖ Database examples: 12 strategies")
        logger.info(
            f"  {'‚úÖ' if llm_connected else '‚ö†Ô∏è'} LLM connection: {'CONNECTED' if llm_connected else 'NOT AVAILABLE'}"
        )
        logger.info(
            f"  {'‚úÖ' if response else '‚è≠Ô∏è'} Strategy generation: {'SUCCESS' if response else 'SKIPPED'}"
        )

        if not llm_connected:
            logger.info(f"\nüí° To enable full testing:")
            logger.info(f"   1. Start LM Studio")
            logger.info(f"   2. Load model: deepseek-r1-0528-qwen3-8b")
            logger.info(f"   3. Start local server")
            logger.info(f"   4. Run this test again")

        logger.info("")

    except Exception as e:
        logger.error(f"\n‚ùå Test failed: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())
